# shared_utils.py
"""
Shared utilities for educational agents.
Contains common helper functions used by both traditional nodes and simulation nodes.
"""

import os
import json
import re
import random
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
from collections import defaultdict
import dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq

# from educational_agent_v1.config_rag import concept_pkg
# from educational_agent_v1.Creating_Section_Text.retriever import retrieve_docs
# from educational_agent_v1.Filtering_GT.filter_utils import filter_relevant_section
# from educational_agent_v1.Creating_Section_Text.schema import NextSectionChoice


dotenv.load_dotenv(dotenv_path=".env", override=True)

# Type alias for AgentState - flexible to work with different state structures
AgentState = Dict[str, Any]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# API Key Performance Tracking
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class ApiKeyPerformanceTracker:
    """Simple tracker for API key performance during load tests.
    
    Stores each call individually and aggregates at export time.
    Thread-safe because list.append() is atomic in CPython.
    """
    
    def __init__(self):
        self._calls = []  # List of all API calls
    
    def record_call(self, api_key_suffix: str, latency_ms: float, success: bool, error_msg: str = None):
        """Record a single API call.
        
        Args:
            api_key_suffix: Last 6 characters of API key
            latency_ms: Latency in milliseconds
            success: Whether call succeeded
            error_msg: Error message if failed
        """
        self._calls.append({
            "api_key": api_key_suffix,
            "latency_ms": latency_ms,
            "success": success,
            "error": error_msg if not success else None,
            "timestamp": time.time()
        })
    
    def get_all_calls(self) -> List[Dict]:
        """Get all recorded calls."""
        return self._calls.copy()
    
    def reset(self):
        """Clear all metrics."""
        self._calls.clear()
        print("üìä API key tracker reset")

# Global singleton instance
_api_key_tracker = ApiKeyPerformanceTracker()

def export_api_key_metrics_to_excel(output_dir: str = "load_tests/reports") -> Optional[str]:
    """Aggregate all API call metrics and export to Excel.
    
    Args:
        output_dir: Directory to save the Excel file
    
    Returns:
        Path to the exported file, or None if no data
    """
    try:
        import pandas as pd
        import numpy as np
        from pathlib import Path
    except ImportError as e:
        print(f"‚ùå Missing required package for Excel export: {e}")
        print("üí° Install with: pip install pandas openpyxl numpy")
        return None
    
    # Get all calls
    all_calls = _api_key_tracker.get_all_calls()
    
    if not all_calls:
        print("‚ö†Ô∏è No API key metrics to export")
        return None
    
    print(f"üìä Aggregating metrics from {len(all_calls)} API calls...")
    
    # Aggregate by API key
    aggregated = defaultdict(lambda: {
        "total_calls": 0,
        "successful_calls": 0,
        "failed_calls": 0,
        "latencies_ms": [],
        "errors": []
    })
    
    for call in all_calls:
        key = call["api_key"]
        aggregated[key]["total_calls"] += 1
        
        if call["success"]:
            aggregated[key]["successful_calls"] += 1
            aggregated[key]["latencies_ms"].append(call["latency_ms"])
        else:
            aggregated[key]["failed_calls"] += 1
            if call["error"]:
                aggregated[key]["errors"].append(call["error"])
    
    # Build DataFrame
    rows = []
    for api_key, data in aggregated.items():
        row = {
            "API Key": f"...{api_key}",
            "Total Calls": data["total_calls"],
            "Successful": data["successful_calls"],
            "Failed": data["failed_calls"],
            "Failure Rate %": round((data["failed_calls"] / data["total_calls"] * 100), 2) if data["total_calls"] > 0 else 0,
        }
        
        # Latency stats (only for successful calls)
        if data["latencies_ms"]:
            latencies = data["latencies_ms"]
            row.update({
                "Min (ms)": round(min(latencies), 2),
                "Max (ms)": round(max(latencies), 2),
                "Avg (ms)": round(np.mean(latencies), 2),
                "P50 (ms)": round(np.percentile(latencies, 50), 2),
                "P95 (ms)": round(np.percentile(latencies, 95), 2),
                "P99 (ms)": round(np.percentile(latencies, 99), 2),
            })
        else:
            # No successful calls
            row.update({
                "Min (ms)": 0,
                "Max (ms)": 0,
                "Avg (ms)": 0,
                "P50 (ms)": 0,
                "P95 (ms)": 0,
                "P99 (ms)": 0,
            })
        
        # Most common error
        if data["errors"]:
            from collections import Counter
            most_common = Counter(data["errors"]).most_common(1)[0][0]
            row["Most Common Error"] = most_common[:50]  # Truncate long errors
        else:
            row["Most Common Error"] = "None"
        
        rows.append(row)
    
    # Create DataFrame and sort by total calls
    df = pd.DataFrame(rows).sort_values("Total Calls", ascending=False)
    
    # Save to Excel
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"api_key_performance_gemma_3_27b_it_{timestamp}.xlsx"
    filepath = Path(output_dir) / filename
    
    df.to_excel(filepath, index=False, engine='openpyxl')
    
    print(f"\nüìä API Key Performance Metrics Exported")
    print(f"üìÅ File: {filepath}")
    print(f"üìà Summary: {len(aggregated)} API keys, {len(all_calls)} total calls")
    print(f"‚úÖ Successful: {sum(d['successful_calls'] for d in aggregated.values())}")
    print(f"‚ùå Failed: {sum(d['failed_calls'] for d in aggregated.values())}")
    
    return str(filepath)

# ‚îÄ‚îÄ‚îÄ Autosuggestion Pool Constants (Single Source of Truth) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Positive affirmations - student understands/agrees
POSITIVE_POOL = [
    "I understand, continue",
    "Yes, got it",
    "That makes sense",
    "Let's proceed further",
    "I'm following along",
    None
]

# Negative/uncertainty - student confused/needs help
NEGATIVE_POOL = [
    "I'm not sure",
    "I don't know",
    "I'm confused",
    "Not very clear",
    "Can you explain differently?"
]

# Special handling - triggers handler logic
SPECIAL_HANDLING_POOL = [
    "Can you give me a hint?",
    "Can you explain that simpler?",
    "Give me an example"
]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def extract_json_block(text: str) -> str:
    """Extract JSON from text, handling various formats including markdown code blocks."""
    s = text.strip()

    # üîç JSON EXTRACTION INPUT üîç
    print("=" * 60)
    print("üîß JSON EXTRACTION - INPUT TEXT")
    print("=" * 60)
    print(f"üìÑ INPUT_LENGTH: {len(s)} characters")
    print(f"üìÑ INPUT_PREVIEW: {s[:200]}...")
    print("=" * 60)

    # 1) Try to find a fenced code block containing JSON (language tag optional)
    m = re.search(r"```(?:json)?\s*({.*?})\s*```", s, flags=re.DOTALL | re.IGNORECASE)
    if m:
        result = m.group(1).strip()
        print("üéØ JSON EXTRACTED - METHOD: Fenced code block")
        print(f"üì¶ EXTRACTED_JSON: {result}")
        print("=" * 60)
        return result

    # 2) Try to find the first balanced JSON object in the text
    start = s.find("{")
    if start != -1:
        depth = 0
        in_str = False
        esc = False
        for i, ch in enumerate(s[start:], start=start):
            if in_str:
                if esc:
                    esc = False
                elif ch == "\\":
                    esc = True
                elif ch == '"':
                    in_str = False
            else:
                if ch == '"':
                    in_str = True
                elif ch == "{":
                    depth += 1
                elif ch == "}":
                    depth -= 1
                    if depth == 0:
                        result = s[start:i+1].strip()
                        print("üéØ JSON EXTRACTED - METHOD: Balanced braces")
                        print(f"üì¶ EXTRACTED_JSON: {result}")
                        print("=" * 60)
                        return result

    # 3) Nothing found ‚Äî return original (let parser raise)
    print("‚ö†Ô∏è JSON EXTRACTION - METHOD: No JSON found, returning original")
    print(f"üì¶ RETURNED_TEXT: {s}")
    print("=" * 60)
    return s


def get_available_api_keys():
    """Get all available Google API keys from environment."""
    api_keys = []
    for i in range(1, 8):  # Check for GOOGLE_API_KEY1 through GOOGLE_API_KEY7
        key = os.getenv(f"GOOGLE_API_KEY_{i}")
        if key:
            api_keys.append(key)
    
    if not api_keys:
        raise RuntimeError("No Google API keys found. Please set GOOGLE_API_KEY1, GOOGLE_API_KEY2, etc. in .env file")
    
    return api_keys


def get_llm(api_key: Optional[str] = None, model: str = "gemma-3-27b-it"):
    """Get configured LLM instance with specified or random API key and model.
    
    Args:
        api_key: Google API key. If None, randomly selects from available keys.
        model: Model name to use. Defaults to gemma-3-27b-it.
    
    Returns:
        Configured ChatGoogleGenerativeAI instance
    """
    if api_key is None:
        # Randomly select an API key from available keys
        available_keys = get_available_api_keys()
        api_key = random.choice(available_keys)
        print(f"üîë Selected random API key (ending with ...{api_key[-6:]})")
    
    print(f"Model being used for LLM calls: {model}")
    
    return ChatGoogleGenerativeAI(
        model=model,
        api_key=api_key,
        temperature=0.5,
    )
    # llm = ChatGroq(
    #     model="llama-3.1-8b-instant",
    #     temperature=0.5,
    #     max_tokens=None,
    # )
    # return llm


# Available models list for fallback
AVAILABLE_GEMINI_MODELS = [
    "gemma-3-27b-it",
    # "gemma-3-27b-it",
    # "gemma-3-27b-it",
    # "gemini-1.5-flash",
]

def invoke_llm_with_fallback(messages: List, operation_name: str = "LLM call", model: str = "gemma-3-27b-it"):
    """
    Invoke LLM with automatic API key and model fallback on failure.
    
    Strategy:
    1. Try all API keys with the selected model
    2. If all keys fail for selected model, try other models from AVAILABLE_GEMINI_MODELS
    3. For each fallback model, try all API keys again
    
    Args:
        messages: List of messages to send to the LLM
        operation_name: Name of the operation for logging purposes
        model: Model to use. Defaults to gemma-3-27b-it.
    
    Returns:
        LLM response object
    
    Raises:
        RuntimeError: If all API keys and models fail
    """
    available_keys = get_available_api_keys()
    
    # Create model priority list: selected model first, then others
    models_to_try = [model] + [m for m in AVAILABLE_GEMINI_MODELS if m != model]
    
    last_error = None
    print("Starting LLM invocation with fallback mechanism with model:", models_to_try[0])
    # Try each model
    for model_idx, current_model in enumerate(models_to_try, 1):
        print(f"üéØ {operation_name} - Trying model {model_idx}/{len(models_to_try)}: {current_model}")
        
        # Shuffle keys for this model to distribute load
        keys_for_model = available_keys.copy()
        random.shuffle(keys_for_model)
        
        # Try each API key for this model
        for attempt, api_key in enumerate(keys_for_model, 1):
            # Get API key suffix for tracking
            api_key_suffix = api_key[-6:]
            
            # Start timing
            start_time = time.time()
            
            try:
                print(f"üîë {operation_name} - Model: {current_model}, API key attempt {attempt}/{len(keys_for_model)} (ending ...{api_key_suffix})")
                llm = get_llm(api_key=api_key, model=current_model)
                response = llm.invoke(messages)
                
                # Calculate latency and record success
                latency_ms = (time.time() - start_time) * 1000
                _api_key_tracker.record_call(api_key_suffix, latency_ms, True)
                
                print(f"‚úÖ {operation_name} - Success with model {current_model} on attempt {attempt} ({latency_ms:.0f}ms)")
                return response
                
            except Exception as e:
                # Calculate latency and record failure
                latency_ms = (time.time() - start_time) * 1000
                last_error = e
                error_msg = str(e)[:100]
                _api_key_tracker.record_call(api_key_suffix, latency_ms, False, error_msg)
                
                print(f"‚ùå {operation_name} - Failed: {error_msg}")
                if attempt < len(keys_for_model):
                    print(f"üîÑ Retrying with next API key for model {current_model}...")
                continue
        
        # All keys failed for this model
        print(f"‚ùå {operation_name} - All API keys failed for model {current_model}")
        if model_idx < len(models_to_try):
            print(f"üîÑ Falling back to next model: {models_to_try[model_idx]}")
    
    # If all keys and models failed, raise the last error
    print(f"‚ùå {operation_name} - All {len(available_keys)} API keys and {len(models_to_try)} models failed!")
    raise RuntimeError(f"All API keys and models exhausted for {operation_name}. Last error: {str(last_error)}") from last_error


def add_ai_message_to_conversation(state: AgentState, content: str):
    """Add AI message to conversation after successful processing."""
    state["messages"].append(AIMessage(content=content))
    print(f"üìù Added AI message to conversation: {content[:50]}...")


def add_system_message_to_conversation(state: AgentState, content: str):
    """Add System message to conversation after successful processing."""
    state["messages"].append(SystemMessage(content=content))
    print(f"üìù Added System message to conversation: {content[:50]}...")


def llm_with_history(state: AgentState, final_prompt: str):
    # üîç LLM INVOCATION - INPUT üîç
    print("=" * 70)
    print("ü§ñ LLM INVOCATION - STARTED")
    print("=" * 70)
    print(f"üìù PROMPT_LENGTH: {len(final_prompt)} characters")
    print(f"üìù PROMPT_PREVIEW: {final_prompt}...")
    print("=" * 70)
    
    # Send the final prompt directly as a human message
    # Note: The final_prompt already contains conversation history via build_prompt_from_template
    request_msgs = [HumanMessage(content=final_prompt)]
    
    # Get model from state, default to gemma-3-27b-it
    model = state.get("model", "gemma-3-27b-it")
    
    # Use the centralized invoke function with fallback
    resp = invoke_llm_with_fallback(request_msgs, operation_name="LLM with history", model=model)
    
    # üîç LLM INVOCATION - OUTPUT üîç
    print("ü§ñ LLM INVOCATION - COMPLETED")
    print(f"üì§ RESPONSE_LENGTH: {len(resp.content)} characters")
    print(f"üì§ RESPONSE_PREVIEW: {resp.content[:200]}...")
    print(f"üìä RESPONSE_TYPE: {type(resp).__name__}")
    print("=" * 70)
    
    # DO NOT append to messages here - let the calling node handle it after parsing
    return resp


def build_conversation_history(state: AgentState) -> str:
    conversation = state.get("messages", [])
    history_text = ""
    
    for msg in conversation:
        if isinstance(msg, HumanMessage) and msg.content == "__start__":
            continue
        elif isinstance(msg, HumanMessage):
            history_text += f"Student: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            history_text += f"Agent: {msg.content}\n"
        elif isinstance(msg, SystemMessage):
            history_text += f"System: {msg.content}\n"
    
    return history_text.strip()


def build_prompt_from_template(system_prompt: str, state: AgentState, 
                             include_last_message: bool = False, 
                             include_instructions: bool = False,
                             parser=None) -> str:
    
    # Add Kannada instruction if needed
    if state.get("is_kannada", False):
        system_prompt += "\n\nIMPORTANT: You must respond ONLY in Kannada language. All your responses must be in Kannada script, not English."
    
    # Build the template string based on what we need
    template_parts = ["{system_prompt}"]
    template_vars = ["system_prompt"]
    
    # Add history if available
    history = build_conversation_history(state)
    if history:
        template_parts.append("\n\nConversation History:\n{history}")
        template_vars.append("history")
    
    # Add last user message if requested
    if include_last_message and state.get("last_user_msg"):
        template_parts.append("\n\nStudent's Latest Response: {last_user_message}")
        template_vars.append("last_user_message")
    
    # Add instructions at the end if requested
    if include_instructions and parser:
        template_parts.append("\n\n{instructions}")
        template_vars.append("instructions")
    
    # Create the template
    template_string = "".join(template_parts)
    prompt_template = PromptTemplate(
        input_variables=template_vars,
        template=template_string
    )
    
    # Prepare the values
    template_values = {"system_prompt": system_prompt}
    
    if history:
        template_values["history"] = history
    
    if include_last_message and state.get("last_user_msg"):
        template_values["last_user_message"] = state["last_user_msg"]
    
    if include_instructions and parser:
        template_values["instructions"] = parser.get_format_instructions()
    
    # Format the prompt
    return prompt_template.format(**template_values)


def build_prompt_from_template_optimized(system_prompt: str, state: AgentState, 
                                       include_last_message: bool = False, 
                                       include_instructions: bool = False,
                                       parser=None, current_node: str = None,
                                       include_autosuggestions: bool = True) -> str:
    
    # Add Kannada instruction if needed
    if state.get("is_kannada", False):
        system_prompt += "\n\nIMPORTANT: You must respond ONLY in Kannada language. All your responses must be in Kannada script, not English."
    
    # Build the template string based on what we need
    template_parts = ["{system_prompt}"]
    template_vars = ["system_prompt"]
    
    # Call history building functions once and reuse the result
    if current_node:
        history = build_node_aware_conversation_history(state, current_node)
    else:
        # Fall back to regular history if no current_node provided
        history = build_conversation_history(state)
    
    # Add history to template if available
    if history:
        template_parts.append("\n\nConversation History:\n{history}")
        template_vars.append("history")
    
    # Add last user message if requested
    if include_last_message and state.get("last_user_msg"):
        print("=====================================================")
        print("Adding last user message to prompt template")
        print("Last user message:", state["last_user_msg"])
        print("=====================================================")
        template_parts.append("\n\nStudent's Latest Response: {last_user_message}")
        template_vars.append("last_user_message")
    
    # Add autosuggestion instructions BEFORE format instructions for pedagogical nodes
    if include_autosuggestions and parser and current_node in ["APK", "CI", "GE", "AR", "TC", "RLC"]:
        # Use imported pool constants for prompt generation
        positive_pool = POSITIVE_POOL
        negative_pool = NEGATIVE_POOL
        special_handling_pool = SPECIAL_HANDLING_POOL
        
        # Get student level and corresponding description
        student_level = state.get("student_level", "medium")
        level_descriptions = {
            "low": "struggling student who needs extra scaffolding, simpler language, and encouragement",
            "medium": "average student progressing normally who needs moderate guidance",
            "advanced": "excelling student ready for deeper challenges and critical thinking"
        }
        level_desc = level_descriptions.get(student_level, level_descriptions["medium"])
        
        template_parts.append(f"""\n\nIMPORTANT - Autosuggestion Generation:

ANALYZE THE CONVERSATION CONTEXT:
- Review the conversation history above carefully
- Consider your current feedback/message that you're about to send
- Determine if your message is a QUESTION or contains "let me think"
- Select autosuggestions that make sense given where the student is in their learning journey
- Make suggestions relevant to what you just explained or asked

üö® CRITICAL: QUESTION/THINKING DETECTION üö®
IF your feedback contains ANY of the following:
  ‚Ä¢ A direct question to the student (e.g., "Can you tell me..?", "What do you think..?", "Why does..?")
  ‚Ä¢ The phrase "let me think"
  ‚Ä¢ Asking student to explain, describe, or answer something

THEN you MUST:
  ‚Ä¢ Set positive_autosuggestion = null/None (not a string, use JSON null)
  ‚Ä¢ Set dynamic_autosuggestion = null/None (not a string, use JSON null)
  ‚Ä¢ Only provide negative_autosuggestion and special_handling_autosuggestion

Reason: When asking questions, we don't want suggestions like "I understand" or exploratory prompts that might distract from answering the question.

SELECTION RULES:

1. **positive_autosuggestion** - CONDITIONAL:
   
   IF your message is NOT a question and does NOT contain "let me think":
       
   - Pick the most contextually appropriate positive/affirmative option from the list below:
     ‚Üí Select EXACTLY ONE from positive pool: {positive_pool}
     ‚ö†Ô∏è WARNING: Only choose from the positive pool above, even if student is confused!
     ‚Üí Pick the most contextually appropriate positive/affirmative option
     ‚Üí These represent what a student COULD say if they understand/agree
   
   IF your message IS a question OR contains "let me think":
     ‚Üí Set to null/Select None (JSON null, not string "null")
   
2. **negative_autosuggestion** - ALWAYS REQUIRED:
   - Pick the most contextually appropriate negative/uncertain option from the list below:
   ‚Üí Select EXACTLY ONE from negative pool: {negative_pool}
       ‚ö†Ô∏è WARNING: Only choose from the negative pool above, even if student is confused!
   ‚Üí Pick the most contextually appropriate negative/uncertain option
   ‚Üí These represent what a student COULD say if they're confused or need help
   
3. **special_handling_autosuggestion** - ALWAYS REQUIRED:
   ‚Üí Select EXACTLY ONE from special handling pool: {special_handling_pool}
   ‚Üí This will trigger special pedagogical intervention (hints, examples, simpler explanation)
   ‚Üí Choose based on what type of help would be most useful given your current message:
     ‚Ä¢ "Can you give me a hint?" - for nudging without revealing answer
     ‚Ä¢ "Can you explain that simpler?" - for complex explanations
     ‚Ä¢ "Give me an example" - for abstract concepts
   
4. **dynamic_autosuggestion** - CONDITIONAL:
   
   IF your message is NOT a question and does NOT contain "let me think":
     ‚Üí Generate EXACTLY ONE unique exploratory suggestion (12-15 words max):
       ‚Ä¢ Must be contextually relevant to the CURRENT conversation and your message
       ‚Ä¢ Should point to a specific unexplored aspect related to what you just explained
       ‚Ä¢ Should nudge student to think about a related concept/application/implication
       ‚Ä¢ Must be DIFFERENT from all pool suggestions above
       ‚Ä¢ Use noun-phrase or question format that evokes curiosity
       
       Adjust depth based on student level ({student_level} - {level_desc}):
       
       ‚Ä¢ low: Concrete, visible aspects
         - Focus on: where it happens, what is used/made, which part does it
         - Example: "Where exactly in the leaf does this happen?"
         - Avoid: abstraction, complex variations, dependencies
       
       ‚Ä¢ medium: Cause-effect, constraints
         - Focus on: why needed, what enables/prevents, usefulness, limitations
         - Example: "Why only green parts of a plant can do this"
         - Balance: not too simple, not overly complex
       
       ‚Ä¢ advanced: Dependencies, variations, implications
         - Focus on: how changes affect outcomes, limiting factors, broader impact
         - Example: "How changes in sunlight intensity affect the rate"
         - Encourage: critical thinking about relationships and constraints
   
   IF your message IS a question OR contains "let me think":
     ‚Üí Set to null/None (JSON null, not string "null")

CRITICAL CONTEXT AWARENESS:
- First, check if your feedback is a question ‚Üí if yes, set positive and dynamic to null
- If you just explained something complex ‚Üí lean towards selecting confused/uncertain options for negative
- If student just answered correctly ‚Üí this is explanatory feedback, provide all 4 suggestions
- If you're asking a challenging question ‚Üí positive and dynamic must be null, only negative and special
- If you're providing encouragement ‚Üí this is feedback, provide all 4 suggestions
- Make suggestions relate to the CURRENT pedagogical moment in the conversation

REMEMBER:
- Questions/"let me think" ‚Üí 2 suggestions (negative + special), positive and dynamic are null
- Explanations/Feedback ‚Üí 4 suggestions (positive + negative + special + dynamic)
- Set as None""")

    
    # Add instructions at the end if requested
    if include_instructions and parser:
        template_parts.append("\n\n{instructions}")
        template_vars.append("instructions")
    
    # Create the template
    template_string = "".join(template_parts)
    prompt_template = PromptTemplate(
        input_variables=template_vars,
        template=template_string
    )
    
    # Prepare the values
    template_values = {"system_prompt": system_prompt}
    
    # Add history if available (already computed above)
    if history:
        template_values["history"] = history
    
    if include_last_message and state.get("last_user_msg"):
        template_values["last_user_message"] = state["last_user_msg"]
    
    if include_instructions and parser:
        template_values["instructions"] = parser.get_format_instructions()
    
    # Format the prompt
    return prompt_template.format(**template_values)


def get_ground_truth(concept: str, section_name: str) -> str:
    # """Retrieve ground truth content for a given concept and section."""
    # try:
    #     # üîç GROUND TRUTH RETRIEVAL - INPUT üîç
    #     print("=" * 70)
    #     print("üìö GROUND TRUTH RETRIEVAL - STARTED")
    #     print("=" * 70)
    #     print(f"üéØ CONCEPT: {concept}")
    #     print(f"üìã SECTION_NAME: {section_name}")
    #     print("=" * 70)
        
    #     # Build a minimal NextSectionChoice object; other fields are dummy since retriever only uses section_name
    #     params = NextSectionChoice(
    #         section_name=section_name,
    #         difficulty=1,
    #         board_exam_importance=1,
    #         olympiad_importance=1,
    #         avg_study_time_min=1,
    #         interest_evoking=1,
    #         curiosity_evoking=1,
    #         critical_reasoning_needed=1,
    #         inquiry_learning_scope=1,
    #         example_availability=1,
    #     )
    #     docs = retrieve_docs(concept, params)
    #     combined = [f"# Page: {d.metadata['page_label']}\n{d.page_content}" for d in docs]
    #     full_doc = "\n---\n".join(combined)
    #     result = filter_relevant_section(concept, section_name, full_doc)
        
    #     # üîç GROUND TRUTH RETRIEVAL - OUTPUT üîç
    #     print("üìö GROUND TRUTH RETRIEVAL - COMPLETED")
    #     print(f"üìÑ DOC_COUNT: {len(docs)} documents")
    #     print(f"üìè FULL_DOC_LENGTH: {len(full_doc)} characters")
    #     print(f"üìè FILTERED_LENGTH: {len(result)} characters")
    #     print(f"üìÑ RESULT_PREVIEW: {result[:300]}...")
    #     print("=" * 70)
        
    #     return result
    # except Exception as e:
    #     print(f"Error retrieving ground truth for {concept} - {section_name}: {e}")
    #     raise
    return ""


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Concept-to-JSON mapping cache
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_CONCEPT_TO_FILE_MAP = None  # Cache for concept-to-file mapping

# Concept alias mapping - maps display names to actual JSON concept names
# This handles cases where the user-facing name differs from the JSON key
_CONCEPT_ALIAS_MAP = {
    "pendulum and its time period": "measurement of time",
    # Add more aliases as needed
}

# Hardcoded section key mapping - covers all possible variations across JSON files
_SECTION_KEY_MAPPING = {
    # Concept definition/description
    "concept definition": ["description", "Description", "desc"],
    
    # Explanation with analogies/intuition
    "explanation (with analogies)": [
        "intuition_logical_flow", 
        "Intuition_Logical_Flow", 
        "Intuition / Logical Flow",
        "intuition / logical flow",
        "Intuition/Logical Flow"
    ],
    
    # Detailed information
    "details (facts, sub-concepts)": ["detail", "Detail", "details", "Details"],
    
    # MCQs - multiple choice questions
    "mcqs": [
        "open_ended_mcqs", 
        "Open-Ended_MCQs", 
        "Open-Ended MCQs",
        "open-ended mcqs",
        "Open Ended MCQs",
        "mcqs",
        "MCQs"
    ],
    
    # Real-life applications
    "real-life application": [
        "real_life_applications", 
        "Real-Life_Applications", 
        "Real-Life Applications",
        "real life applications",
        "Real Life Applications",
        "real-life applications"
    ],
    
    # Working/how it works
    "working": ["working", "Working", "how_it_works", "How_It_Works"],
    
    # Critical thinking
    "critical thinking": [
        "critical_thinking", 
        "Critical_Thinking",
        "critical thinking",
        "Critical Thinking"
    ],
    
    # Key topics from textbook
    "key topics": [
        "key_topics_from_the_textbook",
        "Key_Topics_from_the_Textbook",
        "Key Topics from the Textbook",
        "key topics from the textbook",
        "key_topics",
        "Key_Topics",
        "Key Topics from Textbook"
    ],
    
    # Exam-oriented questions
    "exam questions": [
        "exam_oriented_questions",
        "Exam-Oriented_Questions",
        "Exam-Oriented Questions",
        "exam oriented questions",
        "Exam Oriented Questions"
    ],
    
    # Cross-concept critical thinking
    "cross-concept thinking": [
        "cross_concept_critical_thinking",
        "Cross-Concept_Critical_Thinking",
        "Cross-Concept Critical Thinking",
        "cross concept critical thinking"
    ],
    
    # Relation between sub-concepts
    "relations": [
        "relation_between_sub_concepts",
        "Relation_Between_Sub-Concepts",
        "Relation Between Sub-Concepts",
        "relation between sub-concepts"
    ],
    
    # What-if scenarios
    "what-if scenarios": [
        "what_if_scenarios",
        "What-if_Scenarios",
        "What-If Scenarios",
        "what if scenarios"
    ],
}

def _build_concept_to_file_mapping() -> Dict[str, str]:
    """
    Hardcoded mapping of concept names (lowercase) to their JSON file paths.
    This eliminates the need to scan files on every import, providing instant lookups.
    
    Returns:
        Dict mapping concept names to file paths
    """
    global _CONCEPT_TO_FILE_MAP
    
    if _CONCEPT_TO_FILE_MAP is not None:
        return _CONCEPT_TO_FILE_MAP
    
    # Hardcoded mapping - add new concepts here as needed
    mapping = {
        # 8.json concepts
        "conduction": "NCERT/8.json",
        "convection": "NCERT/8.json",
        "radiation": "NCERT/8.json",
        "good conductors of heat": "NCERT/8.json",
        "poor conductors (insulators) of heat": "NCERT/8.json",
        "land breeze": "NCERT/8.json",
        "sea breeze": "NCERT/8.json",
        "water cycle": "NCERT/8.json",
        "infiltration": "NCERT/8.json",
        "groundwater": "NCERT/8.json",
        
        # 11.json concepts
        "photosynthesis": "NCERT/11.json",
        "chlorophyll": "NCERT/11.json",
        "stomata": "NCERT/11.json",
        "xylem": "NCERT/11.json",
        "phloem": "NCERT/11.json",
        "respiration": "NCERT/11.json",
        "carbon dioxide": "NCERT/11.json",
        "oxygen": "NCERT/11.json",
        "water": "NCERT/11.json",
        "sunlight": "NCERT/11.json",
        
        # 12.json concepts
        "light": "NCERT/12.json",
        "shadows": "NCERT/12.json",
        "reflection": "NCERT/12.json",
        "luminous objects": "NCERT/12.json",
        "non-luminous objects": "NCERT/12.json",
        "transparent materials": "NCERT/12.json",
        "translucent materials": "NCERT/12.json",
        "opaque materials": "NCERT/12.json",
        "image formation": "NCERT/12.json",
        "pinhole camera": "NCERT/12.json",
        
        # NCERT Class 7.json concepts
        "measurement of time": "NCERT/NCERT Class 7.json",
        "history of timekeeping devices": "NCERT/NCERT Class 7.json",
        "sundial": "NCERT/NCERT Class 7.json",
        "water clock": "NCERT/NCERT Class 7.json",
        "hourglass": "NCERT/NCERT Class 7.json",
        "candle clock": "NCERT/NCERT Class 7.json",
        "pendulum and its time period": "NCERT/NCERT Class 7.json",
        "speed": "NCERT/NCERT Class 7.json",
        "uniform motion": "NCERT/NCERT Class 7.json",
        "non-uniform motion": "NCERT/NCERT Class 7.json",
        "si unit of time": "NCERT/NCERT Class 7.json",

        # Fractions.txt concepts
        "fraction as equal share": "NCERT/Fractions.txt",
        "fractional units": "NCERT/Fractions.txt",
        "reading fractions": "NCERT/Fractions.txt",
        "numerator": "NCERT/Fractions.txt",
        "denominator": "NCERT/Fractions.txt",
        "mixed fractions": "NCERT/Fractions.txt",
        "number line": "NCERT/Fractions.txt",
        "equivalent fractions": "NCERT/Fractions.txt",
        "lowest terms": "NCERT/Fractions.txt",
        "brahmagupta's method for adding fractions": "NCERT/Fractions.txt",
        "brahmagupta's method for subtracting fractions": "NCERT/Fractions.txt",
        
        "_default": "NCERT/8.json",
    }
    
    _CONCEPT_TO_FILE_MAP = mapping
    print(f"‚úÖ Loaded hardcoded mapping for {len(mapping)-1} concepts")
    return mapping


def get_all_available_concepts() -> List[str]:
    """
    Get list of all available concepts from the mapping.
    
    Returns:
        List of concept names (properly capitalized for display)
    """
    mapping = _build_concept_to_file_mapping()
    
    # Get all concept keys except the default
    concepts = [key for key in mapping.keys() if key != "_default"]
    
    # Sort alphabetically for better UX
    concepts.sort()
    
    return concepts


def _extract_concept_data_from_json(data: dict, concept: str) -> Optional[dict]:
    """
    Extract concept data from JSON with structure: {"concepts": [{...}, ...]}.
    
    Args:
        data: The loaded JSON data
        concept: The concept name to find
    
    Returns:
        The concept data dict or None if not found
    """
    concept_lower = concept.lower().strip()
    
    # Structure: {"concepts": [{...}, ...]}
    if "concepts" in data and isinstance(data["concepts"], list):
        for concept_data in data["concepts"]:
            concept_name = concept_data.get("concept", concept_data.get("Concept", ""))
            if concept_name.lower().strip() == concept_lower:
                return concept_data
    
    return None


_TEXT_FILE_CACHE = {}

def _parse_text_concept_file(file_path: str) -> Dict[str, Dict[str, str]]:
    """
    Parses a text file with ### Concept: structure into a dictionary.
    Returns: {concept_name_lower: {section_header_lower: content}}
    """
    global _TEXT_FILE_CACHE
    if file_path in _TEXT_FILE_CACHE:
        return _TEXT_FILE_CACHE[file_path]

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        concepts = {}
        # Split by "### Concept:"
        parts = re.split(r'### Concept:\s*', text, flags=re.IGNORECASE)
        
        for part in parts[1:]: # Skip the first part
            lines = part.split('\n')
            concept_name = lines[0].strip().lower()
            
            # Split by "#### " to get sections
            content_text = '\n'.join(lines[1:])
            section_parts = re.split(r'####\s*', content_text)
            
            sections = {}
            for section_part in section_parts:
                if not section_part.strip():
                    continue
                
                section_lines = section_part.split('\n')
                section_header = section_lines[0].strip().lower()
                section_content = '\n'.join(section_lines[1:]).strip()
                
                sections[section_header] = section_content
                
            concepts[concept_name] = sections
            
        _TEXT_FILE_CACHE[file_path] = concepts
        print(f"‚úÖ Parsed and cached text file: {file_path} ({len(concepts)} concepts)")
        return concepts
    except Exception as e:
        print(f"‚ùå Error parsing text file {file_path}: {e}")
        return {}


def get_ground_truth_from_json(concept: str, section_name: str) -> str:
    """
    Retrieve ground truth content from JSON or Text file for a given concept and section.
    Uses cached mapping for fast lookup. No formatting - returns raw content for LLM consumption.
    
    Args:
        concept: The concept name to find
        section_name: The section/key within the concept to retrieve
    
    Returns:
        str: The relevant content from the file
    """
    try:        
        # üîç GROUND TRUTH RETRIEVAL - INPUT üîç
        print("=" * 70)
        print("üìö GROUND TRUTH RETRIEVAL - STARTED")
        print("=" * 70)
        print(f"üéØ CONCEPT: {concept}")
        print(f"üìã SECTION_NAME: {section_name}")
        print("=" * 70)
        
        # Get concept-to-file mapping
        mapping = _build_concept_to_file_mapping()
        concept_key = concept.lower().strip()
        
        print(f"‚úÖ Loaded hardcoded mapping for {len(mapping)} concepts")
        
        # Find the file for this concept
        file_path = mapping.get(concept_key)
        
        if not file_path:
            # Return empty string if concept not found in mapping
            print(f"‚ö†Ô∏è Concept '{concept}' not in mapping, returning empty string")
            print("=" * 70)
            return ""
        
        print(f"üìÇ Found concept in: {file_path}")
        
        # Handle Text files
        if file_path.endswith('.txt'):
            parsed_data = _parse_text_concept_file(file_path)
            
            if concept_key not in parsed_data:
                result = f"Concept '{concept}' not found in text file"
                print(f"‚ùå {result}")
                print("=" * 70)
                return ""
            
            concept_sections = parsed_data[concept_key]
            
            # Use the hardcoded section key mapping
            section_key_mapping = _SECTION_KEY_MAPPING
            
            # Get mapped keys (try multiple possible keys)
            possible_keys = section_key_mapping.get(section_name.lower(), [section_name])
            if not isinstance(possible_keys, list):
                possible_keys = [possible_keys]
            
            # Try each possible key until we find content
            content = None
            used_key = None
            
            # Also try exact match with section_name
            keys_to_try = possible_keys + [section_name]
            
            for key in keys_to_try:
                key_lower = key.lower()
                if key_lower in concept_sections:
                    content = concept_sections[key_lower]
                    used_key = key
                    break
            
            if content is None:
                result = f"Section '{section_name}' not found for concept '{concept}' in text file"
            else:
                result = content
                
        # Handle JSON files
        else:
            # Load the JSON file
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract concept data using the concept name
            concept_data = _extract_concept_data_from_json(data, concept)
            
            if not concept_data:
                result = f"Concept '{concept}' not found in JSON data"
                print(f"‚ùå {result}")
                print("=" * 70)
                return ""
            
            # Use the hardcoded section key mapping
            section_key_mapping = _SECTION_KEY_MAPPING
            
            # Get mapped keys (try multiple possible keys)
            possible_keys = section_key_mapping.get(section_name.lower(), [section_name])
            if not isinstance(possible_keys, list):
                possible_keys = [possible_keys]
            
            # Try each possible key until we find content
            content = None
            used_key = None
            for key in possible_keys:
                if key in concept_data:
                    content = concept_data[key]
                    used_key = key
                    break
            
            if content is None:
                result = f"Section '{section_name}' not found for concept '{concept}'"
            else:
                # Handle different data types but keep minimal processing
                if isinstance(content, list):
                    result = "\n".join([str(item) for item in content]) if content else ""
                elif isinstance(content, dict):
                    result = json.dumps(content, indent=2)  # Pretty print for better LLM parsing
                else:
                    result = str(content) if content else ""
        
        # üîç GROUND TRUTH RETRIEVAL - OUTPUT üîç
        print("üìö GROUND TRUTH RETRIEVAL - COMPLETED")
        print(f"üìã SECTION_KEY_USED: {used_key if 'used_key' in locals() else section_name}")
        print(f"üìè RESULT_LENGTH: {len(result)} characters")
        print(f"üìÑ RESULT_PREVIEW: {result[:200]}...")
        print("=" * 70)
        
        return result
        
    except Exception as e:
        error_msg = f"Error retrieving ground truth for {concept} - {section_name}: Error: {e}, Used file: {file_path if 'file_path' in locals() else 'N/A'}"
        print(f"‚ùå {error_msg}")
        print("=" * 70)
        # return result
        raise RuntimeError(error_msg) from e

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Simulation configuration helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def create_simulation_config(variables: List, concept: str, action_config: Optional[Dict] = None) -> Dict:

    action_config = action_config or {}
    # Default parameters
    base_params = {"length": 1.0, "gravity": 9.8, "amplitude": 75, "mass": 1.0}
    
    # Extract independent variable that's being changed
    independent_var = None
    for var in variables:
        # Handle both Pydantic objects (legacy) and dictionaries (new format)
        if hasattr(var, 'role'):  # Pydantic object
            if var.role == "independent":
                independent_var = var.name.lower()
                break
        elif isinstance(var, dict):  # Dictionary format
            if var.get('role') == "independent":
                independent_var = var.get('name', '').lower()
                break
    
    if not independent_var:
        raise ValueError(f"No independent variable found for concept: {concept}")
    
    # Map concept variables to simulation parameters
    if "length" in independent_var or "length" in concept.lower():
        return {
            "concept": concept,
            "parameter_name": "length",
            "before_params": {**base_params, "length": 1.0},
            "after_params": {**base_params, "length": 3.0},
            "action_description": "increasing the pendulum length from 1.0m to 3.0m",
            "timing": {"before_duration": 8, "transition_duration": 3, "after_duration": 8},
            "agent_message": "Watch how the period changes as I increase the length for you...(Before Time Period was 2.01s and After Time Period is 3.47s)"
        }
    elif "gravity" in independent_var or "gravity" in concept.lower():
        return {
            "concept": concept,
            "parameter_name": "gravity",
            "before_params": {**base_params, "gravity": 9.8},
            "after_params": {**base_params, "gravity": 50.0},  # High gravity demonstration
            "action_description": "changing gravity from Earth (9.8 m/s¬≤) to high gravity (50 m/s¬≤)",
            "timing": {"before_duration": 8, "transition_duration": 3, "after_duration": 8},
            "agent_message": "Watch carefully as I change the gravity for you to see how the period changes...(Before Time Period was 2.01s and After Time Period is 0.89s)"
        }
    elif "amplitude" in independent_var or "angle" in independent_var:
        return {
            "concept": concept,
            "parameter_name": "amplitude",
            "before_params": {**base_params, "amplitude": 30},
            "after_params": {**base_params, "amplitude": 60},
            "action_description": "increasing the starting angle from 30¬∞ to 60¬∞",
            "timing": {"before_duration": 6, "transition_duration": 2, "after_duration": 6},
            "agent_message": "Watch closely as I increase the swing angle for you to see how the period changes...(The time periods will remaain the same as 2.01 seconds before and after)"
        }
    elif "mass" in independent_var or "bob" in independent_var:
        # For pendulum physics, mass doesn't affect the period, but we can demonstrate this
        return {
            "concept": concept,
            "parameter_name": "mass_demo",
            "before_params": {**base_params, "mass": 1},
            "after_params": {**base_params, "mass": 10},  # Same parameters to show no change
            "action_description": "comparing pendulums with different bob masses (but same period)",
            "timing": {"before_duration": 8, "transition_duration": 3, "after_duration": 8},
            "agent_message": "Watch this carefully! I'll show you how changing the bob mass affects the period - this might surprise you!(The time periods will remain the same as 2.01 seconds before and after)"
        }
    elif "frequency" in independent_var or "period" in independent_var:
        # Demonstrate period/frequency by changing length
        return {
            "concept": concept,
            "parameter_name": "length",
            "before_params": {**base_params, "length": 0.5},
            "after_params": {**base_params, "length": 2.0},
            "action_description": "changing length to show how period and frequency are related",
            "timing": {"before_duration": 7, "transition_duration": 3, "after_duration": 7},
            "agent_message": "I'll show you how changing length affects both period and frequency - watch this demonstration..."
        }
    else:
        raise ValueError(f"Unrecognized independent variable '{independent_var}' for concept: {concept}")


def select_most_relevant_image_for_concept_introduction(concept: str, definition_context: str, model: str = "gemma-3-27b-it") -> Optional[Dict]:
    """
    Select the most pedagogically relevant image for introducing a concept.
    Uses the concept-to-file mapping to find the correct JSON file.
    
    Args:
        concept: The concept name (can be in any case)
        definition_context: The context/definition being provided to the student
        model: Model to use for image selection. Defaults to gemma-3-27b-it.
    
    Returns:
        Dict with url, description, and relevance_reason, or None if no images found
    """
    try:
        # Get concept-to-file mapping
        mapping = _build_concept_to_file_mapping()
        concept_key = concept.lower().strip()
        
        # Find the JSON file for this concept
        json_file_path = mapping.get(concept_key)
        
        if not json_file_path:
            print(f"‚ö†Ô∏è Concept '{concept}' not in mapping, cannot retrieve images")
            return None
        
        print(f"üìÇ Looking for images in: {json_file_path}")
        
        # Load the JSON file
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Extract concept data
        concept_data = _extract_concept_data_from_json(data, concept)
        
        if not concept_data:
            print(f"Concept '{concept}' not found in JSON data")
            return None
        
        # Get images from concept data (handle different key variations)
        # 8.json and NCERT Class 7.json use "images" (plural)
        # 11.json and 12.json use "image" (singular)
        available_images = concept_data.get("images", concept_data.get("image", concept_data.get("Images", [])))
        
        if not available_images:
            print(f"No images found for concept '{concept}'")
            return None
        
        # Create LLM prompt for image selection
        images_text = "\n".join([
            f"Image {i+1}: {img.get('description', 'No description')}" 
            for i, img in enumerate(available_images)
        ])
        
        selection_prompt = f"""You are helping select the most pedagogically effective image for introducing the concept "{concept}" to a Class 7 student.

Context being provided to student:
{definition_context}

Available images:
{images_text}

Select the image that would be MOST helpful for a 12-13 year old student to understand this concept during the definition phase.

Consider:
- Visual clarity and simplicity
- Direct relevance to the core concept
- Age-appropriate complexity
- Ability to reinforce the definition

Respond with JSON only:
{{
    "selected_image_number": <1-based index>,
    "relevance_reason": "<2-3 sentences explaining why this image is best for concept introduction>"
}}"""
        
        # Get LLM response with fallback
        response = invoke_llm_with_fallback(
            [HumanMessage(content=selection_prompt)],
            operation_name="Image selection",
            model=model
        )
        
        # Parse response
        json_text = extract_json_block(response.content)
        selection_data = json.loads(json_text)
        
        selected_index = selection_data.get("selected_image_number", 1) - 1  # Convert to 0-based
        
        if 0 <= selected_index < len(available_images):
            selected_image = available_images[selected_index]
            return {
                "url": selected_image.get("url", ""),
                "description": selected_image.get("description", ""),
                "relevance_reason": selection_data.get("relevance_reason", "This image was selected as most relevant for concept introduction.")
            }
        else:
            print(f"Invalid image selection index: {selected_index}")
            return None
            
    except Exception as e:
        print(f"Error selecting image for concept '{concept}': {e}")
        import traceback
        traceback.print_exc()
        return None


# ‚îÄ‚îÄ‚îÄ Memory Optimization Functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def identify_node_segments_from_transitions(messages: list, transitions: list) -> list:
    """
    Split messages into segments based on recorded node transitions.
    Transition happens AFTER the agent response, so messages belong to the 'from_node'.
    """
    if not transitions:
        # No transitions recorded, treat all messages as one segment  
        return [{"node": "unknown", "messages": messages, "start_idx": 0, "end_idx": len(messages)}]
    
    segments = []
    start_idx = 0
    
    for transition in transitions:
        # Messages up to (and including) transition point belong to 'from_node'
        end_idx = transition["transition_after_message_index"] 
        
        if end_idx > start_idx:
            segments.append({
                "node": transition["from_node"],
                "messages": messages[start_idx:end_idx],
                "start_idx": start_idx,
                "end_idx": end_idx
            })
        start_idx = end_idx
    
    # Add the final segment (current node messages) - messages after last transition
    if start_idx < len(messages):
        current_node = transitions[-1]["to_node"] if transitions else "current"
        segments.append({
            "node": current_node,
            "messages": messages[start_idx:], 
            "start_idx": start_idx,
            "end_idx": len(messages)
        })
    
    return segments

def create_educational_summary(messages: list, model: str = "gemma-3-27b-it") -> str:
    """
    Use LLM to create a proper educational summary of the conversation.
    
    Args:
        messages: List of conversation messages
        model: Model to use for summarization. Defaults to gemma-3-27b-it.
    """
    if not messages:
        return ""
    
    # Extract agent messages for summarization
    agent_messages = [msg.content for msg in messages if isinstance(msg, AIMessage)]
    student_messages = [msg.content for msg in messages if isinstance(msg, HumanMessage)]
    
    if not agent_messages:
        return f"Student made {len(student_messages)} responses"
    
    # Build conversation text for summarization
    conversation_text = ""
    for msg in messages:
        if isinstance(msg, HumanMessage):
            conversation_text += f"Student: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            conversation_text += f"Agent: {msg.content}\n"
    
    # Limit conversation text to avoid token overflow
    if len(conversation_text) > 2000:
        conversation_text = conversation_text[:2000] + "..."
    
    # Use LLM to summarize
    summary_prompt = f"""Summarize the following educational conversation in 2-3 sentences, focusing on:
- What concept was being taught
- Student's understanding level
- Key pedagogical interactions

Conversation:
{conversation_text}

Summary:"""
    
    
    try:
        summary_response = invoke_llm_with_fallback(
            [HumanMessage(content=summary_prompt)],
            operation_name="Educational summary",
            model=model
        )
        return summary_response.content.strip()
    except Exception as e:
        print(f"‚ùå Error creating LLM summary: {e}")
        # Fallback to simple summary if LLM fails
        return f"Educational discussion with {len(messages)} exchanges about the concept"

def create_educational_summary_from_text(conversation_text: str, model: str = "gemma-3-27b-it") -> str:
    """
    Create an LLM-generated summary from conversation text.
    
    Args:
        conversation_text: Text of the conversation to summarize
        model: Model to use for summarization. Defaults to gemma-3-27b-it.
    """
    try:
        if not conversation_text.strip():
            return "Empty conversation segment"
        
        # Limit conversation text to avoid token overflow
        if len(conversation_text) > 2000:
            conversation_text = conversation_text[:2000] + "..."
        
        # Create educational summary prompt
        summary_prompt = f"""Summarize the following educational conversation in 2-3 sentences, focusing on:
- What concept was being taught
- Student's understanding level  
- Key pedagogical interactions

Conversation:
{conversation_text}

Summary:"""

        summary_response = invoke_llm_with_fallback(
            [HumanMessage(content=summary_prompt)],
            operation_name="Summary from text",
            model=model
        )
        return summary_response.content.strip()
    except Exception as e:
        print(f"‚ùå Error creating LLM summary: {e}")
        # Fallback to simple summary if LLM fails
        return "Educational discussion about the concept"

def build_node_aware_conversation_history(state: AgentState, current_node: str) -> str:
    """
    Keep exact messages from current and previous node interactions.
    Use cached summaries and only summarize new content incrementally.
    """
    messages = state.get("messages", [])
    transitions = state.get("node_transitions", [])
    model = state.get("model", "gemma-3-27b-it")
    
    # For short conversations, use full history
    if len(messages) <= 6:
        return build_conversation_history(state)
    
    # Get node segments based on recorded transitions
    segments = identify_node_segments_from_transitions(messages, transitions)
    
    print(f"üìä MEMORY OPTIMIZATION: Found {len(segments)} node segments")
    
    if len(segments) >= 2:
        # Keep current + previous node segments exact
        current_segment = segments[-1]  # Current node
        previous_segment = segments[-2]  # Previous node
        older_segments = segments[:-2]   # Everything before previous node
        
        print(f"üìä Current node: {current_segment['node']} ({len(current_segment['messages'])} messages)")
        print(f"üìä Previous node: {previous_segment['node']} ({len(previous_segment['messages'])} messages)")
        print(f"üìä Older segments: {len(older_segments)} segments")
        
        # Handle summary efficiently
        summary = ""
        
        if older_segments:
            # Calculate what needs to be summarized
            older_messages = []
            for segment in older_segments:
                older_messages.extend(segment["messages"])
            
            # Get last older index directly from segment metadata (O(1) operation)
            last_older_index = older_segments[-1]["end_idx"] - 1 if older_segments else -1
            
            # Check if we need to update summary
            if last_older_index <= state.get("summary_last_index", -1):
                # Use existing summary - no new messages to summarize
                summary = state.get("summary", "")
                print(f"üìä ‚úÖ Using existing summary (covers up to index {state.get('summary_last_index', -1)})")
            else:
                # Need to update summary with new messages
                new_messages_start = state.get("summary_last_index", 0) + 1
                new_messages = messages[new_messages_start:last_older_index + 1]

                if state.get("summary", ""):
                    # Combine old summary with new messages
                    print(f"Old Summary: {state.get('summary')}")
                    combined_content = f"Previous summary: {state.get('summary')}\n\nNew messages:\n"
                    for msg in new_messages:
                        if isinstance(msg, HumanMessage):
                            combined_content += f"Student: {msg.content}\n"
                        elif isinstance(msg, AIMessage):
                            combined_content += f"Agent: {msg.content}\n"
                    
                    print(f"üìä üîÑ Updating summary: old summary + {len(new_messages)} new messages...")
                    summary = create_educational_summary_from_text(combined_content, model=model)
                else:
                    # First time - just summarize the messages
                    print(f"üìä üîÑ Creating first summary for {len(new_messages)} messages...")
                    summary = create_educational_summary(new_messages, model=model)
                
                # Update summary state
                state["summary"] = summary
                state["summary_last_index"] = last_older_index
                print(f"üìä üíæ Updated summary (now covers up to index {last_older_index})")
            
            summary = f"Previous conversation summary: {summary}\n\n"
        
        # Format recent messages (previous + current node) exactly
        recent_messages = previous_segment["messages"] + current_segment["messages"]
        recent_text = ""
        for msg in recent_messages:
            if isinstance(msg, HumanMessage):
                recent_text += f"Student: {msg.content}\n"
            elif isinstance(msg, AIMessage):
                recent_text += f"Agent: {msg.content}\n"
        
        optimized_history = summary + recent_text.strip()
        print(f"üìä OPTIMIZATION RESULT: {len(build_conversation_history(state))} -> {len(optimized_history)} chars")
        return optimized_history
    
    else:
        # Not enough transitions, fall back to regular history
        print(f"üìä Not enough transitions, using full history")
        return build_conversation_history(state)

def reset_memory_summary(state: AgentState):
    """
    Reset the memory summary. Useful for testing or manual management.
    """
    if "summary" in state:
        del state["summary"]
        del state["summary_last_index"]
        print("üìä üóëÔ∏è Memory summary reset")

# ‚îÄ‚îÄ‚îÄ Pedagogical‚Äêmove context (shared between traditional and simulation nodes) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

